{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I: Image Collection via Web Scraping\n",
    "\n",
    "In the first stage of the project, the goal was to collect images from websites, with a special focus on identifying images that could correspond to company logos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "dataset_path = './dataset/'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)\n",
    "else:\n",
    "    print(f\"The folder '{dataset_path}' already exists.\")\n",
    "\n",
    "with open(\"logs.txt\", \"a\") as logs_file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_logo_from_html(domain)` function was used to extract images from the webpages of the specified domains.\n",
    "\n",
    "### 1. Accessing URLs\n",
    "The function attempts to access multiple variations of URLs for each domain to ensure that a functional URL is found:\n",
    "- `https://www.{domain}`\n",
    "- `http://www.{domain}`\n",
    "- `https://{domain}`\n",
    "- `http://{domain}`\n",
    "\n",
    "### 2. Searching for Relevant Images\n",
    "After obtaining the HTML page, the function uses **BeautifulSoup** to parse the content and search for all `<img>` tags, which contain the sources of the images. In this way, all images from the website are obtained. However, since not all images are automatically logos, a filter is applied based on the following conditions to save only the images that appear to be logos:\n",
    "\n",
    "- **Searching within the `<header>` tag**:\n",
    "  - If the image is found within a `<header>` tag, which is often used for visible sections of websites (where logos are typically located), and if the image's `alt` attribute contains the word \"logo\" or if the image file name contains the words \"logo\" or \"favicon,\" the image is considered relevant.\n",
    "\n",
    "- **`<link>` tags with `rel='icon'` or `rel='shortcut icon'` attributes**:\n",
    "  - These tags are used to specify the site favicons, which are usually logos. These images are also added to the list of relevant images.\n",
    "\n",
    "- **Manually adding `/favicon.ico`**:\n",
    "  - By default, `/favicon.ico` is added to the list of images, as this is the standard location for favicons used by many sites.\n",
    "\n",
    "- **Checking Image Dimensions**:\n",
    "  - It is required that the dimensions of these images be smaller than 512px in both width and height, as logos typically have smaller dimensions.\n",
    "\n",
    "### 3. Saving Images to the Dataset\n",
    "Finally, only the images from the website that meet the conditions listed above are saved to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(url, file_name, dataset_path):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.49 Safari/537.36 Edge/112.0.1722.64',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if 'favicon.ico' not in url:\n",
    "                with open(\"logs.txt\", \"a\") as logs_file:\n",
    "                    logs_file.write(f\"Error accessing {url}: {e}\\n\")\n",
    "            return None\n",
    "\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        valid_types = ['image/png', 'image/jpeg', 'image/jpg', 'image/webp', 'image/gif', 'image/x-icon', 'image/vnd.microsoft.icon']\n",
    "        if content_type not in valid_types:\n",
    "            return None\n",
    "\n",
    "        image_data = BytesIO(response.content)\n",
    "        try:\n",
    "            image = Image.open(image_data)\n",
    "            image.verify()\n",
    "            image = Image.open(image_data)\n",
    "        except UnidentifiedImageError:\n",
    "            return None\n",
    "\n",
    "        width, height = image.size\n",
    "        if width > 512 or height > 512:\n",
    "            return None\n",
    "\n",
    "        image = image.convert(\"RGBA\")\n",
    "        file_path = os.path.join(dataset_path, file_name + \".png\")\n",
    "        image.save(file_path, format=\"PNG\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(\"logs.txt\", \"a\") as logs_file:\n",
    "            logs_file.write(f\"Error downloading the image from {url}: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logo_from_html(domain):\n",
    "    response = None\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.5615.49 Safari/537.36 Edge/112.0.1722.64',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept': 'image/x-icon, image/*;q=0.8, */*;q=0.5'\n",
    "        }\n",
    "\n",
    "        urls = [\n",
    "            f'https://www.{domain}',\n",
    "            f'http://www.{domain}',\n",
    "            f'https://{domain}',\n",
    "            f'http://{domain}'\n",
    "        ]\n",
    "\n",
    "        correctUrl = ''\n",
    "        for url in urls:\n",
    "            headers['Referer'] = url\n",
    "            response = requests.get(url, verify=False, headers=headers, timeout=10)\n",
    "                    \n",
    "            if response.status_code == 403:\n",
    "                correctUrl = url\n",
    "                break\n",
    "\n",
    "            response.raise_for_status()\n",
    "            correctUrl = url\n",
    "            break\n",
    "\n",
    "        if response is None:\n",
    "            with open(\"logs.txt\", \"a\") as logs_file:\n",
    "                logs_file.write(f\"All requests failed for {domain}\\n\")\n",
    "            return None, []\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            found_images = []\n",
    "            images = soup.find_all('img')\n",
    "            for image in images:\n",
    "                image_url = image.get('src')\n",
    "                alt_text = image.get('alt', '').lower()\n",
    "                file_name = os.path.basename(image_url) if image_url else ''\n",
    "\n",
    "                if image.find_parent('header') and (\n",
    "                    'logo' in alt_text or \n",
    "                    'logo' in file_name.lower() or \n",
    "                    'favicon' in file_name.lower()\n",
    "                ):\n",
    "                    found_images.append(image_url)\n",
    "\n",
    "            link_tags = soup.find_all('link', rel='icon') + soup.find_all('link', rel='shortcut icon')\n",
    "            for link in link_tags:\n",
    "                found_images.append(link.get('href'))\n",
    "            \n",
    "            found_images.append('/favicon.ico')\n",
    "            \n",
    "            found_images = list(set(filter(None, found_images)))\n",
    "            return correctUrl, found_images\n",
    "        \n",
    "        except Exception as e:\n",
    "            with open(\"logs.txt\", \"a\") as logs_file:\n",
    "                logs_file.write(f\"Parsing error for {domain}: {e}\\n\")\n",
    "            return None, []\n",
    "    except Exception as e:\n",
    "        with open(\"logs.txt\", \"a\") as logs_file:\n",
    "            logs_file.write(f\"Error processing domain {domain}: {e}\\n\")\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process was repeated for each domain in the dataset to collect as many images that appeared to be logos as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_domains(domains):\n",
    "    size = len(domains)\n",
    "\n",
    "    print(f\"Starting to process {size} domains...\")\n",
    "\n",
    "    for i, domain in enumerate(domains):\n",
    "        file_name = domain + '_' + str(i)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing {i + 1}/{size} domains...\")\n",
    "\n",
    "        try:\n",
    "            url, logo_urls = get_logo_from_html(domain)\n",
    "            \n",
    "            for j, logo_url in enumerate(logo_urls):\n",
    "                logo_url = urljoin(url, logo_url)\n",
    "                save_image(logo_url, f\"{file_name}_{j}\", dataset_path)\n",
    "        except Exception as e:\n",
    "            with open(\"logs.txt\", \"a\") as logs_file:\n",
    "                logs_file.write(f\"Error processing domain {domain}: {e}\\n\")\n",
    "\n",
    "    print(\"Processing complete. All domains have been processed.\")\n",
    "\n",
    "df = pd.read_parquet(\"logos.snappy.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "domains = df['domain']\n",
    "\n",
    "scrape_from_domains(domains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
